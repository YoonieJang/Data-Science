{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-26T11:00:36.266166Z","iopub.execute_input":"2021-10-26T11:00:36.266532Z","iopub.status.idle":"2021-10-26T11:00:36.284614Z","shell.execute_reply.started":"2021-10-26T11:00:36.266472Z","shell.execute_reply":"2021-10-26T11:00:36.283638Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Load Dataset","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv('../input/emotions-dataset-for-nlp/train.txt',names=['sentence','emotion'],header=None, sep=';')\ntest_data = pd.read_csv('../input/emotions-dataset-for-nlp/test.txt',names=['sentence','emotion'],header=None, sep=';')\nval_data= pd.read_csv('../input/emotions-dataset-for-nlp/val.txt',names=['sentence','emotion'],header=None, sep=';')\nprint('Total data:',train_data.shape)\nprint('Total data:',test_data.shape)\nprint('Total data:',val_data.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-26T11:00:36.289811Z","iopub.execute_input":"2021-10-26T11:00:36.290085Z","iopub.status.idle":"2021-10-26T11:00:36.349244Z","shell.execute_reply.started":"2021-10-26T11:00:36.290053Z","shell.execute_reply":"2021-10-26T11:00:36.347981Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"print('Train data Null Check:\\n',train_data.isnull().sum(),'\\n')\nprint('Test data Null Check:\\n',test_data.isnull().sum(),'\\n')\nprint('Validation data Null Check:\\n',val_data.isnull().sum())\n","metadata":{"execution":{"iopub.status.busy":"2021-10-26T11:00:36.737224Z","iopub.execute_input":"2021-10-26T11:00:36.737802Z","iopub.status.idle":"2021-10-26T11:00:36.755499Z","shell.execute_reply.started":"2021-10-26T11:00:36.737766Z","shell.execute_reply":"2021-10-26T11:00:36.754706Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_data.emotion.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-10-26T11:00:38.084637Z","iopub.execute_input":"2021-10-26T11:00:38.085017Z","iopub.status.idle":"2021-10-26T11:00:38.100359Z","shell.execute_reply.started":"2021-10-26T11:00:38.084977Z","shell.execute_reply":"2021-10-26T11:00:38.099017Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\ntrain_data['length'] = train_data['sentence'].apply(len) # number of characters\nplt.figure(figsize=(10,7))\nsns.kdeplot(x=train_data[\"length\"], hue=train_data[\"emotion\"])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-26T11:00:40.117305Z","iopub.execute_input":"2021-10-26T11:00:40.118230Z","iopub.status.idle":"2021-10-26T11:00:41.107256Z","shell.execute_reply.started":"2021-10-26T11:00:40.118184Z","shell.execute_reply":"2021-10-26T11:00:41.106063Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_data['length'].max()","metadata":{"execution":{"iopub.status.busy":"2021-10-26T11:00:41.109140Z","iopub.execute_input":"2021-10-26T11:00:41.109651Z","iopub.status.idle":"2021-10-26T11:00:41.115930Z","shell.execute_reply.started":"2021-10-26T11:00:41.109614Z","shell.execute_reply":"2021-10-26T11:00:41.114918Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# Encode target labels with value between 0 and n_classes-1.\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ntrain_data['emotion'] = le.fit_transform(train_data['emotion'])\ntest_data['emotion'] = le.fit_transform(test_data['emotion'])\nval_data['emotion'] = le.fit_transform(val_data['emotion'])\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-26T11:00:43.830250Z","iopub.execute_input":"2021-10-26T11:00:43.830844Z","iopub.status.idle":"2021-10-26T11:00:43.897584Z","shell.execute_reply.started":"2021-10-26T11:00:43.830807Z","shell.execute_reply":"2021-10-26T11:00:43.896602Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"test_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-26T11:01:00.299950Z","iopub.execute_input":"2021-10-26T11:01:00.300709Z","iopub.status.idle":"2021-10-26T11:01:00.310608Z","shell.execute_reply.started":"2021-10-26T11:01:00.300644Z","shell.execute_reply":"2021-10-26T11:01:00.309887Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import re\nimport nltk\nnltk.download('wordnet')\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nstopwords = set(nltk.corpus.stopwords.words('english'))\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-10-26T11:01:04.244151Z","iopub.execute_input":"2021-10-26T11:01:04.245080Z","iopub.status.idle":"2021-10-26T11:01:04.256373Z","shell.execute_reply.started":"2021-10-26T11:01:04.244999Z","shell.execute_reply":"2021-10-26T11:01:04.255295Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nfrom bs4 import BeautifulSoup\nimport string\nvocab_size= 100000\nlen_sentence = 150\n\ndef decontracted(phrase):\n    \"\"\"\n    We first define a function to expand the contracted phrase into normal words\n    \"\"\"\n    # specific\n    phrase = re.sub(r\"wont\", \"will not\", phrase)\n    phrase = re.sub(r\"wouldnt\", \"would not\", phrase)\n    phrase = re.sub(r\"shouldnt\", \"should not\", phrase)\n    phrase = re.sub(r\"couldnt\", \"could not\", phrase)\n    phrase = re.sub(r\"cudnt\", \"could not\", phrase)\n    phrase = re.sub(r\"cant\", \"can not\", phrase)\n    phrase = re.sub(r\"dont\", \"do not\", phrase)\n    phrase = re.sub(r\"doesnt\", \"does not\", phrase)\n    phrase = re.sub(r\"didnt\", \"did not\", phrase)\n    phrase = re.sub(r\"wasnt\", \"was not\", phrase)\n    phrase = re.sub(r\"werent\", \"were not\", phrase)\n    phrase = re.sub(r\"havent\", \"have not\", phrase)\n    phrase = re.sub(r\"hadnt\", \"had not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\ t\", \" not\", phrase)\n    #phrase = re.sub(r\"\\re\", \" are\", phrase)\n    phrase = re.sub(r\"\\ s \", \" is \", phrase) # prime \n    phrase = re.sub(r\"\\ d \", \" would \", phrase)\n    phrase = re.sub(r\"\\ ll \", \" will \", phrase)\n    phrase = re.sub(r\"\\dunno\", \"do not \", phrase)\n    phrase = re.sub(r\"ive \", \"i have \", phrase)\n    phrase = re.sub(r\"im \", \"i am \", phrase)\n    phrase = re.sub(r\"i m \", \"i am \", phrase)\n    phrase = re.sub(r\" w \", \" with \", phrase)\n    \n    return phrase\n\ndef preprocessing(df):\n    \"\"\"\n    Clean the review texts\n    \"\"\"\n    cleaned_review = []    \n    stemmer = PorterStemmer()\n\n    for review_text in tqdm(df['sentence']):\n        \n        # expand the contracted words\n        review_text = decontracted(review_text)\n        \n        #remove html tags\n        review_text = BeautifulSoup(review_text, 'lxml').get_text().strip() # re.sub(r'<.*?>', '', text)\n        \n        #remove non-alphabetic characters\n        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n    \n        #remove url \n        review_text = re.sub(r'https?://\\S+|www\\.\\S+', '', review_text)\n        \n        #Removing punctutation, string.punctuation in python consists of !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~`\n        review_text = review_text.translate(str.maketrans('', '', string.punctuation))\n\n        # remove emails\n        review_text = re.sub(r\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\", '', review_text)\n        review_text = review_text.split()\n        review_text = [stemmer.stem(word) for word in review_text if word not in stopwords]\n        review_text = \" \".join(review_text)\n    \n        cleaned_review.append(review_text)\n        \n    return cleaned_review","metadata":{"execution":{"iopub.status.busy":"2021-10-26T11:01:05.503282Z","iopub.execute_input":"2021-10-26T11:01:05.503966Z","iopub.status.idle":"2021-10-26T11:01:05.635895Z","shell.execute_reply.started":"2021-10-26T11:01:05.503918Z","shell.execute_reply":"2021-10-26T11:01:05.634963Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_data['cleaned_data'] = preprocessing(train_data)\ntest_data['cleaned_data'] = preprocessing(test_data)\nval_data['cleaned_data'] = preprocessing(val_data)","metadata":{"execution":{"iopub.status.busy":"2021-10-26T11:01:07.391264Z","iopub.execute_input":"2021-10-26T11:01:07.391600Z","iopub.status.idle":"2021-10-26T11:01:21.538829Z","shell.execute_reply.started":"2021-10-26T11:01:07.391563Z","shell.execute_reply":"2021-10-26T11:01:21.537907Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"test_data['cleaned_data'][0]","metadata":{"execution":{"iopub.status.busy":"2021-10-26T11:01:22.631888Z","iopub.execute_input":"2021-10-26T11:01:22.632195Z","iopub.status.idle":"2021-10-26T11:01:22.638840Z","shell.execute_reply.started":"2021-10-26T11:01:22.632163Z","shell.execute_reply":"2021-10-26T11:01:22.637820Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def encode(text):\n    one_hot_words = [one_hot(input_text=word, n=10000) for word in text]\n    padded = pad_sequences(sequences = one_hot_words, maxlen =len_sentence, padding =\"pre\") # padding is used to provide uniformity in the sentences.\n    print(text.shape)\n    return padded","metadata":{"execution":{"iopub.status.busy":"2021-10-26T11:01:28.056595Z","iopub.execute_input":"2021-10-26T11:01:28.056920Z","iopub.status.idle":"2021-10-26T11:01:28.064366Z","shell.execute_reply.started":"2021-10-26T11:01:28.056886Z","shell.execute_reply":"2021-10-26T11:01:28.063032Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"x_train = encode(train_data['cleaned_data'])\nx_val = encode(val_data['cleaned_data'])\nx_test = encode(test_data['cleaned_data'])","metadata":{"execution":{"iopub.status.busy":"2021-10-26T11:01:29.001553Z","iopub.execute_input":"2021-10-26T11:01:29.002312Z","iopub.status.idle":"2021-10-26T11:01:29.470447Z","shell.execute_reply.started":"2021-10-26T11:01:29.002270Z","shell.execute_reply":"2021-10-26T11:01:29.469530Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# labels\ny_train = train_data['emotion']\ny_val = val_data['emotion']\ny_test = test_data['emotion']","metadata":{"execution":{"iopub.status.busy":"2021-10-26T11:01:34.903561Z","iopub.execute_input":"2021-10-26T11:01:34.903904Z","iopub.status.idle":"2021-10-26T11:01:34.910011Z","shell.execute_reply.started":"2021-10-26T11:01:34.903869Z","shell.execute_reply":"2021-10-26T11:01:34.909101Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder \n# What is One_hot in Python?\n#One-hot encoding is essentially the representation of categorical variables as binary vectors.\nonehot_encoder = OneHotEncoder()\ny_train = np.array(y_train)\ny_train = onehot_encoder.fit_transform(y_train.reshape(-1,1)).toarray()\nprint(y_train)\n\ny_val = np.array(y_val)\ny_val = onehot_encoder.fit_transform(y_val.reshape(-1,1)).toarray()\nprint(y_val)\n\ny_test = np.array(y_test)\ny_test = onehot_encoder.fit_transform(y_test.reshape(-1,1)).toarray()\nprint(y_test)","metadata":{"execution":{"iopub.status.busy":"2021-10-26T11:01:36.562062Z","iopub.execute_input":"2021-10-26T11:01:36.562451Z","iopub.status.idle":"2021-10-26T11:01:36.579911Z","shell.execute_reply.started":"2021-10-26T11:01:36.562415Z","shell.execute_reply":"2021-10-26T11:01:36.578758Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Building a model","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","metadata":{"execution":{"iopub.status.busy":"2021-10-26T11:01:47.917100Z","iopub.execute_input":"2021-10-26T11:01:47.917424Z","iopub.status.idle":"2021-10-26T11:01:47.922985Z","shell.execute_reply.started":"2021-10-26T11:01:47.917389Z","shell.execute_reply":"2021-10-26T11:01:47.921801Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"#building the neural network\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, output_dim = 150,input_length = len_sentence), # gives an output of the shape of a 2D array w/ length of the sentence as one diemntion and the embedding dimention\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(64,activation='sigmoid'),\n    tf.keras.layers.Dense(6,activation='softmax')\n])\nmodel.compile(loss = 'categorical_crossentropy',optimizer = 'adam',metrics=['accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-10-26T11:12:16.484409Z","iopub.execute_input":"2021-10-26T11:12:16.485573Z","iopub.status.idle":"2021-10-26T11:12:16.697188Z","shell.execute_reply.started":"2021-10-26T11:12:16.485514Z","shell.execute_reply":"2021-10-26T11:12:16.696159Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"callbacks = [\n    keras.callbacks.EarlyStopping(\n    monitor = \"val_loss\",\n        min_delta=1e-2,\n        patience = 2,\n        verbose=1\n    )\n]","metadata":{"execution":{"iopub.status.busy":"2021-10-26T11:12:18.131834Z","iopub.execute_input":"2021-10-26T11:12:18.132164Z","iopub.status.idle":"2021-10-26T11:12:18.138064Z","shell.execute_reply.started":"2021-10-26T11:12:18.132128Z","shell.execute_reply":"2021-10-26T11:12:18.136913Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"num_epoch = 15\nhistory = model.fit(x_train, y_train, epochs = num_epoch,batch_size = 64, callbacks=callbacks,\n                   validation_data = (x_val, y_val),verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-26T11:12:19.375913Z","iopub.execute_input":"2021-10-26T11:12:19.376406Z","iopub.status.idle":"2021-10-26T11:14:35.931268Z","shell.execute_reply.started":"2021-10-26T11:12:19.376369Z","shell.execute_reply":"2021-10-26T11:14:35.930114Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## Model Evaluation","metadata":{}},{"cell_type":"code","source":"results = model.evaluate(x_test,y_test)\nprint(results)","metadata":{"execution":{"iopub.status.busy":"2021-10-26T11:14:44.744910Z","iopub.execute_input":"2021-10-26T11:14:44.745205Z","iopub.status.idle":"2021-10-26T11:14:45.042820Z","shell.execute_reply.started":"2021-10-26T11:14:44.745175Z","shell.execute_reply":"2021-10-26T11:14:45.042067Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"history_dict = history.history\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'bo', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-26T11:14:47.164628Z","iopub.execute_input":"2021-10-26T11:14:47.164961Z","iopub.status.idle":"2021-10-26T11:14:47.670074Z","shell.execute_reply.started":"2021-10-26T11:14:47.164925Z","shell.execute_reply":"2021-10-26T11:14:47.669012Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"plt.clf()   # clear figure\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-26T11:14:49.825263Z","iopub.execute_input":"2021-10-26T11:14:49.825585Z","iopub.status.idle":"2021-10-26T11:14:50.093765Z","shell.execute_reply.started":"2021-10-26T11:14:49.825549Z","shell.execute_reply":"2021-10-26T11:14:50.092482Z"},"trusted":true},"execution_count":32,"outputs":[]}]}